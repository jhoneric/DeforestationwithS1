{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3325d8",
   "metadata": {},
   "source": [
    "# Detecting Deforestation Using Sentinel -1 Data (Cusum Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4d272",
   "metadata": {},
   "source": [
    "This notebook is used for creating the TWO MAIN PRODUCTS as result of applying the Cumulative Sum algorithm to a *Sentinel-1 Dataset*.\n",
    "\n",
    "**Explanation: Here** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65442361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import ee\n",
    "import os\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triger the GEE API\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3154148",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Although Sentinel-1 from the GEE python API has already some pre-processing, the literature recommends to apply some additional steps.\n",
    "\n",
    "\n",
    "1. Reduce Speckle.\n",
    "\n",
    "2. Convert to gamma zero values ( $\\gamma^0$.).\n",
    "\n",
    "3. Create the RFDI(Radar forest deforestation index) band.\n",
    "\n",
    "4. Create mosaics for the same date over the AOI \n",
    "\n",
    "5. Create the image collections for each band used in the processing stage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1227ba",
   "metadata": {},
   "source": [
    "### 1. Reduce Speckle.\n",
    "#### Functions to apply to the images before create the timeseries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be151e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to reduce speckle \n",
    "# taken from https://github.com/adugnag/gee_s1_ard/\n",
    "\n",
    "# RefinedLee filter\n",
    "def RefinedLee(img):\n",
    "   \n",
    "    bandNames = img.bandNames().remove('angle')\n",
    "\n",
    "    def inner(b):\n",
    "\n",
    "        scene = img.select([b]);\n",
    "    \n",
    "        # scene must be linear, i.e. not in dB!\n",
    "        # Set up 3x3 kernels \n",
    "        \n",
    "        weights3 = ee.List.repeat(ee.List.repeat(1,3),3);\n",
    "        kernel3 = ee.Kernel.fixed(3,3, weights3, 1, 1, False);\n",
    "  \n",
    "        mean3 = scene.reduceNeighborhood(ee.Reducer.mean(), kernel3);\n",
    "        variance3 = scene.reduceNeighborhood(ee.Reducer.variance(), kernel3);\n",
    "  \n",
    "        # Use a sample of the 3x3 windows inside a 7x7 windows to determine gradients and directions\n",
    "        sample_weights = ee.List([[0,0,0,0,0,0,0], [0,1,0,1,0,1,0],[0,0,0,0,0,0,0], [0,1,0,1,0,1,0], [0,0,0,0,0,0,0], [0,1,0,1,0,1,0],[0,0,0,0,0,0,0]]);\n",
    "  \n",
    "        sample_kernel = ee.Kernel.fixed(7,7, sample_weights, 3,3, False);\n",
    "  \n",
    "        # Calculate mean and variance for the sampled windows and store as 9 bands\n",
    "        sample_mean = mean3.neighborhoodToBands(sample_kernel); \n",
    "        sample_var = variance3.neighborhoodToBands(sample_kernel);\n",
    "  \n",
    "        # Determine the 4 gradients for the sampled windows\n",
    "        gradients = sample_mean.select(1).subtract(sample_mean.select(7)).abs();\n",
    "        gradients = gradients.addBands(sample_mean.select(6).subtract(sample_mean.select(2)).abs());\n",
    "        gradients = gradients.addBands(sample_mean.select(3).subtract(sample_mean.select(5)).abs());\n",
    "        gradients = gradients.addBands(sample_mean.select(0).subtract(sample_mean.select(8)).abs());\n",
    "  \n",
    "        # And find the maximum gradient amongst gradient bands\n",
    "        max_gradient = gradients.reduce(ee.Reducer.max());\n",
    "  \n",
    "        # Create a mask for band pixels that are the maximum gradient\n",
    "        gradmask = gradients.eq(max_gradient);\n",
    "  \n",
    "        # duplicate gradmask bands: each gradient represents 2 directions\n",
    "        gradmask = gradmask.addBands(gradmask);\n",
    "  \n",
    "        # Determine the 8 directions\n",
    "        directions = sample_mean.select(1).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(7))).multiply(1);\n",
    "        directions = directions.addBands(sample_mean.select(6).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(2))).multiply(2));\n",
    "        directions = directions.addBands(sample_mean.select(3).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(5))).multiply(3));\n",
    "        directions = directions.addBands(sample_mean.select(0).subtract(sample_mean.select(4)).gt(sample_mean.select(4).subtract(sample_mean.select(8))).multiply(4));\n",
    "        \n",
    "        # The next 4 are the not() of the previous 4\n",
    "        directions = directions.addBands(directions.select(0).Not().multiply(5));\n",
    "        directions = directions.addBands(directions.select(1).Not().multiply(6));\n",
    "        directions = directions.addBands(directions.select(2).Not().multiply(7));\n",
    "        directions = directions.addBands(directions.select(3).Not().multiply(8));\n",
    "  \n",
    "        # Mask all values that are not 1-8\n",
    "        directions = directions.updateMask(gradmask);\n",
    "  \n",
    "        # \"collapse\" the stack into a singe band image (due to masking, each pixel has just one value (1-8) in it's directional band, and is otherwise masked)\n",
    "        directions = directions.reduce(ee.Reducer.sum());  \n",
    "  \n",
    "        sample_stats = sample_var.divide(sample_mean.multiply(sample_mean));\n",
    "  \n",
    "        #Calculate localNoiseVariance\n",
    "        sigmaV = sample_stats.toArray().arraySort().arraySlice(0,0,5).arrayReduce(ee.Reducer.mean(), [0]);\n",
    "  \n",
    "        # Set up the 7*7 kernels for directional statistics\n",
    "        rect_weights = ee.List.repeat(ee.List.repeat(0,7),3).cat(ee.List.repeat(ee.List.repeat(1,7),4));\n",
    "  \n",
    "        diag_weights = ee.List([[1,0,0,0,0,0,0], [1,1,0,0,0,0,0], [1,1,1,0,0,0,0], [1,1,1,1,0,0,0], [1,1,1,1,1,0,0], [1,1,1,1,1,1,0], [1,1,1,1,1,1,1]]);\n",
    "  \n",
    "        rect_kernel = ee.Kernel.fixed(7,7, rect_weights, 3, 3, False);\n",
    "        diag_kernel = ee.Kernel.fixed(7,7, diag_weights, 3, 3, False);\n",
    "  \n",
    "        # Create stacks for mean and variance using the original kernels. Mask with relevant direction.\n",
    "        dir_mean = scene.reduceNeighborhood(ee.Reducer.mean(), rect_kernel).updateMask(directions.eq(1));\n",
    "        dir_var = scene.reduceNeighborhood(ee.Reducer.variance(), rect_kernel).updateMask(directions.eq(1));\n",
    "  \n",
    "        dir_mean = dir_mean.addBands(scene.reduceNeighborhood(ee.Reducer.mean(), diag_kernel).updateMask(directions.eq(2)));\n",
    "        dir_var = dir_var.addBands(scene.reduceNeighborhood(ee.Reducer.variance(), diag_kernel).updateMask(directions.eq(2)));\n",
    "  \n",
    "        # and add the bands for rotated kernels\n",
    "        for i in range(1, 4):\n",
    "            dir_mean = dir_mean.addBands(scene.reduceNeighborhood(ee.Reducer.mean(), rect_kernel.rotate(i)).updateMask(directions.eq(2*i+1)))\n",
    "            dir_var = dir_var.addBands(scene.reduceNeighborhood(ee.Reducer.variance(), rect_kernel.rotate(i)).updateMask(directions.eq(2*i+1)))\n",
    "            dir_mean = dir_mean.addBands(scene.reduceNeighborhood(ee.Reducer.mean(), diag_kernel.rotate(i)).updateMask(directions.eq(2*i+2)))\n",
    "            dir_var = dir_var.addBands(scene.reduceNeighborhood(ee.Reducer.variance(), diag_kernel.rotate(i)).updateMask(directions.eq(2*i+2)))\n",
    "\n",
    "  \n",
    "        # \"collapse\" the stack into a single band image (due to masking, each pixel has just one value in it's directional band, and is otherwise masked)\n",
    "        dir_mean = dir_mean.reduce(ee.Reducer.sum());\n",
    "        dir_var = dir_var.reduce(ee.Reducer.sum());\n",
    "  \n",
    "        # A finally generate the filtered value\n",
    "        varX = dir_var.subtract(dir_mean.multiply(dir_mean).multiply(sigmaV)).divide(sigmaV.add(1.0))\n",
    "  \n",
    "        b = varX.divide(dir_var)\n",
    "        result = dir_mean.add(b.multiply(scene.subtract(dir_mean)))\n",
    "  \n",
    "        return result.arrayProject([0]).arrayFlatten([['sum']]).float()\n",
    "    \n",
    "    result = ee.ImageCollection(bandNames.map(inner)).toBands().rename(bandNames).copyProperties(img)\n",
    "    \n",
    "    return img.addBands(result, None, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e1252",
   "metadata": {},
   "source": [
    "### 2. Convert to gamma zero values ( $\\gamma^0$.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert sigma to gamma zero\n",
    "# taken from DETER SAR github\n",
    "def toGamma0(img):\n",
    "    \n",
    "    vv_gamma0=img.select('VV').subtract(img.select('angle').multiply(np.pi/180.0).cos().log10().multiply(10.0))\n",
    "    vh_gamma0=img.select('VH').subtract(img.select('angle').multiply(np.pi/180.0).cos().log10().multiply(10.0))\n",
    "    \n",
    "    return img.addBands(vv_gamma0.rename('VVg0')).addBands(vh_gamma0.rename('VHg0')).copyProperties(img).copyProperties(img,['system:time_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert db to power scale (recommended by SAR Handbook)\n",
    "def toPower(img):\n",
    "    \n",
    "    base = 10.0\n",
    "    \n",
    "    #operation\n",
    "    \n",
    "    vv_power = ee.Image(base).pow(img.select('VVg0').divide(base)).rename('VV_pow')\n",
    "    vh_power = ee.Image(base).pow(img.select('VHg0').divide(base)).rename('VH_pow')\n",
    "    \n",
    "    return img.addBands(vv_power).addBands(vh_power).copyProperties(img).copyProperties(img,['system:time_start'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e02ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert power scale to db (recommended by SAR Handbook)\n",
    "\n",
    "def toDb(img):\n",
    "    \n",
    "    base = 10\n",
    "    \n",
    "    VV_fdb = ee.Image(base).multiply(img.select('VV_pow').log10()).rename('VV_fdb')\n",
    "    VH_fdb = ee.Image(base).multiply(img.select('VH_pow').log10()).rename('VH_fdb')\n",
    "    \n",
    "    return img.addBands(VV_fdb).addBands(VH_fdb).copyProperties(img).copyProperties(img,['system:time_start'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558811f",
   "metadata": {},
   "source": [
    "### 3. Create the RFDI(Radar forest deforestation index) band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the RFDI band\n",
    "def rfdi(img):\n",
    "    num = img.select('VV_pow').subtract(img.select('VH_pow'))\n",
    "    den = img.select('VV_pow').add(img.select('VH_pow'))\n",
    "    rfdi = num.divide(den).rename('rfdi')\n",
    "    return img.addBands(rfdi).copyProperties(img).copyProperties(img,['system:time_start'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425be87",
   "metadata": {},
   "source": [
    "### 4. Create mosaics for the same date over the AOI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create mosaic with the same date in the AOI extend\n",
    "def mosaicByDate(imgs):\n",
    "    imgList = imgs.toList(imgs.size())\n",
    "    unique_dates = imgList.map(lambda im: ee.Image(im).date().format(\"YYYY-MM-dd\")).distinct()\n",
    "    \n",
    "    def mosaic_imlist(d):\n",
    "        d = ee.Date(d)\n",
    "        im = imgs.filterDate(d, d.advance(1, \"day\")).mosaic() \n",
    "        return im.set(\n",
    "        \"system:time_start\", d.millis(), \n",
    "        \"system:id\", d.format(\"YYYY-MM-dd\"));\n",
    "\n",
    "    return ee.ImageCollection(unique_dates.map(mosaic_imlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411915b",
   "metadata": {},
   "source": [
    "### 5. Create the image collections for each band used in the processing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb5c5e",
   "metadata": {},
   "source": [
    "#### Period of study and AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining period of study\n",
    "\n",
    "startDate = \"2021-01-01\"; #no image before\n",
    "endDate = \"2022-09-01\"; #exclusive = ie., until 2022-09-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fac1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area of interest\n",
    "#studyArea = ee.FeatureCollection(\"projects/ee-geeric/assets/data/AOI\") #loaded from GEE assests\n",
    "#studyArea = ee.FeatureCollection(\"projects/ee-geeric/assets/data/test_region_forest\")\n",
    "#studyArea = ee.FeatureCollection(\"projects/ee-geeric/assets/data/test_region_river\")\n",
    "studyArea = ee.FeatureCollection(\"projects/ee-geeric/assets/data/subset1\")\n",
    "\n",
    "\n",
    "\n",
    "# choose a path for your output directory. This is where you will have saved all the images in the collection\n",
    "\n",
    "outdir = '/home/sepal-user/My_files/Outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583e7de",
   "metadata": {},
   "source": [
    "#### Aplying functions and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63256fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the S1 Image Collection\n",
    "\n",
    "S1 = (ee.ImageCollection('COPERNICUS/S1_GRD')\n",
    "          .filterBounds(studyArea).filterDate(ee.Date(startDate), ee.Date(endDate))\n",
    "          .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
    "          .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) # only descending as ascending images are not available for 2022\n",
    "          .filterMetadata('transmitterReceiverPolarisation', 'equals', ['VV', 'VH'])\n",
    "          .filterMetadata('resolution_meters', 'equals', 10)\n",
    "          .map(lambda image:image.clip(studyArea.geometry()))\n",
    "          .map(toGamma0))\n",
    "\n",
    "\n",
    "countS1 = S1.size()\n",
    "print('Number of images in the collection: ', str(countS1.getInfo())+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the original VVg0 and VHg0 for further operations\n",
    "S1g0=S1.select(['VVg0', 'VHg0'])\n",
    "\n",
    "# Apply conversion to power before speckle reduction\n",
    "S1g0_pow=S1g0.map(toPower)\n",
    "\n",
    "#Apply refinedLee filter for reducing speckle\n",
    "S1g0_N_filter = S1g0_pow.select(['VV_pow', 'VH_pow']).map(RefinedLee)\n",
    "\n",
    "# Get the RFDI band and the VV and VH filtered bands in db\n",
    "S1g0_index = S1g0_N_filter.map(rfdi).map(toDb)\n",
    "\n",
    "#Combine collections\n",
    "S1processed = S1g0.combine(S1g0_index)\n",
    "print(\"created bands: \" + str (S1processed.first().bandNames().getInfo()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4de6df",
   "metadata": {},
   "source": [
    "### Create the timeseries dataset by band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a15bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select and filter the collections by band\n",
    "\n",
    "S1_vvf = S1processed.select(['VV_fdb'])\n",
    "\n",
    "S1_vhf = S1processed.select(['VH_fdb'])\n",
    "\n",
    "S1_vvg0 = S1processed.select(['VVg0'])\n",
    "\n",
    "S1_vhg0 = S1processed.select(['VHg0'])\n",
    "\n",
    "S1_rfdi = S1processed.select(['rfdi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939822ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting unnecessary objects\n",
    "del S1, countS1, S1g0, S1g0_N_filter, S1g0_index, S1processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the collections to lists and get the dates\n",
    "\n",
    "collections =[S1_vvf, S1_vhf, S1_vvg0, S1_vhg0, S1_rfdi]\n",
    "list_c = []\n",
    "list_dates = []\n",
    "\n",
    "for c in collections:\n",
    "   \n",
    "    ctol = c.toList(c.size())\n",
    "    unique_dates = ctol.map(lambda im:ee.Image(im).date().format(\"YYYY-MM-dd\")).distinct()\n",
    "    date_list = unique_dates.getInfo()\n",
    "    #date_list.sort()\n",
    "\n",
    "    #variable_name = [k for k, v in locals().items() if v == c][0] \n",
    "    #newname = 'list_'+variable_name\n",
    "    \n",
    "    list_c.append(ctol)\n",
    "    list_dates.append(date_list)\n",
    "    \n",
    "    \n",
    "#----------------------------------- \n",
    "S1_vv_list = list_c[0] \n",
    "S1_vh_list = list_c[1]  \n",
    "S1_vvg0_list = list_c[2] \n",
    "S1_vhg0_list = list_c[3] \n",
    "S1_rfdi_list = list_c[4] \n",
    "\n",
    "S1_vv_dates = list_dates[0] \n",
    "S1_vh_dates = list_dates[1] \n",
    "S1_vvg0_dates = list_dates[2] \n",
    "S1_vhg0_dates = list_dates[3] \n",
    "S1_rfdi_dates = list_dates[4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning memory\n",
    "del S1_vv_list, S1_vh_list, S1_vvg0_list, S1_vhg0_list, S1_vh_dates, S1_vvg0_dates, S1_vhg0_dates##, S1_rfdi_dates, S1_rfdi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b331ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of scenes: \" + str(S1_rfdi.size().getInfo()))\n",
    "print(\"number of dates: \" + str(len(S1_rfdi_dates)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90257b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed140950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mosaicking sneces into the AOI with the same date\n",
    "vvf_mosaic = mosaicByDate(S1_rfdi)\n",
    "mosaic_size = vvf_mosaic.size().getInfo()\n",
    "print('size of Image Stack after mosaicking: ' ,mosaic_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b1cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vvf_mosaic.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aed215",
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_dates, list_c,collections, S1_vv_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4018f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4664d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export images to collection folder\n",
    "\n",
    "outdir = 'D:\\\\NOVAIMS\\\\TESIS_PROJECT\\\\Data\\\\Outputs\\\\rfdi_sub1'\n",
    "\n",
    "crs = \"EPSG:4326\"\n",
    "geemap.ee_export_image_collection(vvf_mosaic, scale=30,out_dir=outdir, region=studyArea.geometry(),crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f996db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all downloaded images\n",
    "import glob\n",
    "outdir = 'D:\\\\NOVAIMS\\\\TESIS_PROJECT\\\\Data\\\\Outputs\\\\rfdi_sub1'\n",
    "#vv_tifs = os.listdir(outdir)\n",
    "#files = list(filter(os.path.isfile, glob.glob(outdir + \"*.tif\")))\n",
    "files = glob.glob(outdir+\"/*.tif\")  \n",
    "#sort images by download time\n",
    "files.sort(key=lambda x: os.path.getctime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_da=[]\n",
    "for file, date in zip(files, S1_rfdi_dates):\n",
    "    da = xr.open_rasterio(file,masked=True)\n",
    "    dt = datetime.datetime.strptime(date,\"%Y-%m-%d\")\n",
    "    dt = pd.to_datetime(dt)\n",
    "    da = da.assign_coords(time = dt)\n",
    "    da = da.expand_dims(dim=\"time\")\n",
    "    list_da.append(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493721f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack data arrays in list\n",
    "ts_vv = xr.combine_by_coords(list_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0081d1",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "The processing steps includes the operations for Cumulative Sums algorithm.\n",
    "\n",
    "**Include here the formula behind the algorthm**\n",
    "\n",
    "\n",
    "1. Convert gee collection to xarray\n",
    "\n",
    "2. Get time series mean\n",
    "\n",
    "3. Get the residuals for the timeseries\n",
    "\n",
    "4. Apply the cumulative sum of the residuals \n",
    "\n",
    "5. Get the max of the Cusum for each pixel\n",
    "\n",
    "6. Find a threshold for the values considered as change\n",
    "\n",
    "7. Mask the values considered as change (deforestation)\n",
    "\n",
    "8. Export from xarray to geotiff the pixels considered as deforestation\n",
    "\n",
    "9. Get the dates where the deforestation was found\n",
    "\n",
    "10. Export from xarray to geotiff where pixel values indicate the date of deforestation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90969d8b",
   "metadata": {},
   "source": [
    "### 1. Convert gee collection to xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wxee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac227c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "wxee.Initialize()\n",
    "crs = \"EPSG:4326\"\n",
    "scale = 30  # pixel size in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22765762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be done for each band (collecion)\n",
    "ts_vv = S1_rfdi.wx.to_xarray(scale=scale, crs=crs)\n",
    "\n",
    "#ts_vv = vvf_mosaic.wx.to_xarray(scale=scale, crs=crs)\n",
    "\n",
    "#ts_vh = S1_vvf.wx.to_xarray(scale=scale, crs=crs)  # run just one collection at time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336076fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd09c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_vv = ts_vv.groupby(\"time.date\").mean(dim=\"time\")\n",
    "#mosaic_vv = ts_vv.groupby(\"time.date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_vv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76403ac",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Get time series mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b964aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get timeseries mean\n",
    "dsmean = ts_vv.mean(dim='time', keep_attrs=True) #replace the time series Xarraydataset for other bands\n",
    "#dsmean = mosaic_vv.mean(dim='date', keep_attrs=True) #replace the time series Xarraydataset for other bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b6616",
   "metadata": {},
   "source": [
    "### 3. Get the residuals for the timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get time series residual\n",
    "Res = ts_vv-dsmean # replace the time series Xarraydataset for other bands\n",
    "#Res = mosaic_vv-dsmean # replace the time series Xarraydataset for other bands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b7b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9694240",
   "metadata": {},
   "source": [
    "### 4. Apply the cumulative sum of the residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c24a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get time series cumulative sum\n",
    "#from Arraydataset to DataArray\n",
    "Residuals = Res\n",
    "#Residuals = Res.to_array()\n",
    "S = Residuals.cumsum(dim=\"time\")\n",
    "#S = Residuals.cumsum(dim=\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bfab7",
   "metadata": {},
   "source": [
    "### 5. Get the max of the Cusum for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab41a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximum of the cumulative sum\n",
    "#Smax= S.max(dim=\"time\")  \n",
    "Smax= S.max(dim=\"date\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get minimum of the cusum\n",
    "Smin = S.min(dim='time')\n",
    "#Smin = S.min(dim='date')\n",
    "\n",
    "#get the diff\n",
    "Sdiff = S.max(dim ='time') - S.min(dim ='time')\n",
    "#Sdiff = S.max(dim ='date') - S.min(dim ='date')\n",
    "\n",
    "\n",
    "#checkin the diff in specific location\n",
    "ts_diff = Sdiff.isel(y = 35 , x = 102)\n",
    "ts_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd9767",
   "metadata": {},
   "source": [
    "### 6. Find a threshold for the values considered as change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70559acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the threshold is calculated as 90th percentile of the CuSum max\n",
    "#the thershold retrieves the pixels considered as change, every pixel where Smax is over percentile 90th\n",
    "#threshold = np.percentile(Smax, 90)\n",
    "\n",
    "#threshold = np.percentile(Sdiff, 90)\n",
    "\n",
    "\n",
    "threshold = np.percentile(Smin, 10)\n",
    "threshold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ds.isel(time=(ds.time.dt.month == 1))\n",
    "#da_f.sel(x=0)\n",
    "\n",
    "# spatially filter by 90th percentile\n",
    "#S90 = S.where(S>= threshold,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c8de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to datetime64 \n",
    "S['date'] = pd.DatetimeIndex(S['date'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aabae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the time series, we are only interested in deforestation after 2022-01-01,\n",
    "#but the data before it, serves as historical background. We captured that background with the cusum  \n",
    "\n",
    "year = 2022 # year of interest\n",
    "\n",
    "S_doi = S.isel(time = (S.time.dt.year == year))\n",
    "#S_doi = S.isel(date = (S.date.dt.year == year))\n",
    "S_doi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the monutoring period to day of the year\n",
    "S_doi['time'] = S_doi['time.dayofyear']\n",
    "#S_doi['date'] = S_doi['date.dayofyear']\n",
    "S_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially filter by 90th percentile . Preserve S values where those S values greather than Threshold\n",
    "#S90 = S_doi.where(S_doi>= threshold,np.nan)\n",
    "S90 = S_doi.where(S_doi<= threshold,np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23c778",
   "metadata": {},
   "source": [
    "### 7. Mask the values considered as change (deforestation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = S90['time'].isel(time=0).notnull()\n",
    "#masked = S90['date'].isel(date=0).notnull()\n",
    "masked\n",
    "\n",
    "#convert Nan to -9999\n",
    "S90filt = S90.fillna(-9999)\n",
    "\n",
    "\n",
    "# get the date where the curve reaches the maximum value\n",
    "S90filt_max = S90filt.isel(time = S90filt.argmax('time')).where(masked)\n",
    "#S90filt_max = S90filt.isel(date = S90filt.argmax('date')).where(masked)\n",
    "\n",
    "\n",
    "# create the array with the maximum values for the cumulative sums for each pixel\n",
    "\n",
    "maxValues = S90filt_max.where(S90filt_max> -9999,np.nan)\n",
    "datesOfMax = S90.idxmax(dim=\"time\")\n",
    "#datesOfMax = S90.idxmax(dim=\"date\")\n",
    "\n",
    "maxValues.name = 'cusummax'\n",
    "datesOfMax.name = 'dates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a051ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datesOfMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192edfb",
   "metadata": {},
   "source": [
    "### 8. Export from xarray to geotiff the pixels considered as deforestation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathMaxValues = 'D:\\\\NOVAIMS\\\\TESIS_PROJECT\\\\Data\\\\Outputs\\\\rfdi_sub1\\\\Smin_rfdi_sub1.tif'   # path to your folder for Smax intensity image\n",
    "\n",
    "tifCusumMax = maxValues.rio.write_crs(crs)\n",
    "tifCusumMax.rio.to_raster(pathMaxValues,compress='LZMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathDatesRaster = 'D:\\\\NOVAIMS\\\\TESIS_PROJECT\\\\Data\\\\Outputs\\\\rfdi_sub1\\\\Dates_rfdi_sub1.tif'       # path to your folder for Date image\n",
    "\n",
    "tifDates = datesOfMax.rio.write_crs(crs)\n",
    "tifDates.rio.to_raster(pathDatesRaster,compress='LZMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fda173",
   "metadata": {},
   "source": [
    "### 9. Get the dates where the deforestation was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert DdatesOfMax from xarray to pd dataframe\n",
    "\n",
    "dates_df = datesOfMax.to_dataframe()\n",
    "\n",
    "#reset the index\n",
    "r_dates_df = dates_df.reset_index()\n",
    "del dates_df\n",
    "\n",
    "#reorganizing the index using X and Y\n",
    "\n",
    "dates_df2 = r_dates_df.set_index(['y','x'])\n",
    "\n",
    "del r_dates_df\n",
    "\n",
    "#drop the column variable(the name of the band), this avoids the error of having two varibles as aditional dimensions of the array\n",
    "dates_df3 = dates_df2.drop(['variable'], axis=1)\n",
    "\n",
    "del dates_df2\n",
    "\n",
    "#convert to datetime64 to string\n",
    "\n",
    "dates_df4 = dates_df3['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "del dates_df3\n",
    "\n",
    "#convert from pandas series to pandas dataframe\n",
    "dates_df5 = dates_df4.to_frame()\n",
    "\n",
    "del dates_df4\n",
    "\n",
    "#function to convert from YYYY-mm-dd to float YYYY,yyyy\n",
    "\n",
    "def YearAndFraction(row):\n",
    "            \n",
    "    if row[\"date\"] in S1_vv_dates:\n",
    "        \n",
    "        year = float (row[0][0:4])            \n",
    "        print(row[0])\n",
    "    \n",
    "        month = float (row[0][5:7])\n",
    "        \n",
    "        day = float (row[0][8:10])\n",
    "        \n",
    "        frm = (month*(1/12))-(1/12)\n",
    "\n",
    "        frd = (day/365)\n",
    "        value = year+frm+frd\n",
    "        print(value)\n",
    "        dates_df5['date'] = dates_df5['date'].replace(row['date'], value)\n",
    "\n",
    "dates_df5.apply(YearAndFraction, axis=1)\n",
    "\n",
    "#convert from pandas df to Xarray dataset\n",
    "datesDetection = dates_df5.to_xarray()\n",
    "# to DataArray\n",
    "datesOfChange=datesDetection.to_array()\n",
    "datesOfChange\n",
    "\n",
    "del dates_df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495941e",
   "metadata": {},
   "source": [
    "### 10. Export from xarray to geotiff where pixel values indicate the date of deforestation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathDatesRaster = '/home/sepal-user/My_files/Outputs/Dates_v.tif'       # path to your folder for Date image\n",
    "\n",
    "tifDates = datesOfChange.rio.write_crs(crs)\n",
    "tifDates.rio.to_raster(pathDatesRaster,compress='LZMA')"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
